<p>
  <a href="{{site.lc_site}}">Library Carpentry</a>
  is made by people working in library- and information-related roles to help you:
</p>
<ul>
  <li>automate repetitive, boring, error-prone tasks</li>
  <li>create, maintain and analyze sustainable and reusable data</li>
  <li>work effectively with IT and systems colleagues</li>
  <li>better understand the use of software in research</li>
  <li>and much more...</li>
</ul>
<p align="center">
  <em>
    Library Carpentry introduces you to the fundamentals of computing
    and provides you with a platform for further self-directed learning.
    For more information on what we teach and why, please see our paper
    "<a href="https://doi.org/10.18352/lq.10176">Library Carpentry: software skills training for library professionals</a>".
  </em>
</p>

<p>
  Web scraping is the process of extracting data from websites. 
  Some data that is available on the web is presented in a format that makes it easier to collect and use it, 
  for example in the form of downloadable comma-separated values (CSV) datasets that can then be imported in a spreadsheet or loaded into a data analysis script. 
  Often however, even though it is publicly available, data is not readily available for reuse. For example it can be contained in a PDF, or a table on a website, 
  or spread across multiple web pages. <br/>    
  There are a variety of ways to scrape a website to extract information for reuse. 
  In its simplest form, this can be achieved by copying and pasting snippets from a web page, b
  ut this can be unpractical if there is a large amount of data to be extracted, or if it spread over a large number of pages. 
  Instead, specialized tools and techniques can be used to automate this process, by defining what sites to visit, what information to look for, 
  and whether data extraction should stop once the end of a page has been reached, or whether to follow hyperlinks and repeat the process recursively. 
  Automating web scraping also allows to define whether the process should be run at regular intervals and capture changes in the data.
</p>
